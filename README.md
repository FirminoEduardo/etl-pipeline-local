# ğŸš€ ETL Pipeline Local com Qualidade de Dados (Data Quality)

Este projeto Ã© um pipeline de Engenharia de Dados construÃ­do do zero para simular um fluxo real de ponta a ponta (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga), focando fortemente em **boas prÃ¡ticas de engenharia de software**, testes automatizados e orquestraÃ§Ã£o.

## ğŸ—ï¸ Arquitetura do Projeto

O pipeline simula a ingestÃ£o de dados de um sistema transacional, convertendo os dados para um formato analÃ­tico colunar (Parquet) e aplicando regras de negÃ³cios rigorosas validadas por testes unitÃ¡rios.

1. **Origem (Banco de Dados):** Container Docker rodando PostgreSQL.
2. **GeraÃ§Ã£o de Dados:** Script Python injeta dados fictÃ­cios de clientes e scores de crÃ©dito.
3. **ExtraÃ§Ã£o (Extract):** Leitura otimizada via SQLAlchemy e Pandas.
4. **ValidaÃ§Ã£o (Data Quality):** Bateria de testes com `pytest` para garantir que a regra de negÃ³cio nÃ£o quebre.
5. **TransformaÃ§Ã£o (Transform):** Filtro analÃ­tico isolando clientes com Score >= 800.
6. **Destino (Load):** Salvamento local particionado em formato `.parquet` usando `pyarrow`.

## ğŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3.11 (Isolado via `pyenv` / `venv`)
* **Banco de Dados:** PostgreSQL (via Docker)
* **Processamento de Dados:** Pandas, Pyarrow
* **Testes e Qualidade:** Pytest
* **OrquestraÃ§Ã£o:** GNU Make (Makefile)

## âš™ï¸ PrÃ©-requisitos

Para rodar este projeto na sua mÃ¡quina, vocÃª precisarÃ¡ de:
* Linux/macOS com `make` instalado.
* Docker instalado e rodando.
* Python 3.11+ e `pip`.

## ğŸš€ Como Executar

Este projeto utiliza um `Makefile` para orquestrar todos os scripts, facilitando a execuÃ§Ã£o sem precisar lembrar a ordem de cada comando.

1. **Clone o repositÃ³rio e instale as dependÃªncias:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install pandas pyarrow sqlalchemy psycopg2-binary faker pytest
